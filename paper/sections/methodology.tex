\section{Methodology}
\label{sec:methodology}

\subsection{Overview}
Our approach combines educational text analysis with LLM-based evaluation in two tracks.
\textbf{Track A} investigates which linguistic and structural features make Wikipedia
articles effective for learning. \textbf{Track B} develops a ``Wiki Diagnostic Tutor'' that
applies those insights to classify and explain article quality. Together, these tracks
address the research question: \emph{How can AI evaluate the pedagogical value of
open-access texts such as Wikipedia?}

\subsection{Data Collection}
For \textbf{Track A}, we performed a systematic review of research on instructional
text design, focusing on coherence, scaffolding, and conceptual density. Based on these
findings, we compiled a labeled dataset of Wikipedia articles that have been rated for
educational quality by prior work or expert reviewers. Each article’s revision ID and
access date were recorded for reproducibility.

For \textbf{Track B}, we used the Wikipedia API, accessed via the OpenAI Apps SDK
\cite{openaiAppsSDK}, to dynamically retrieve article content. The prototype fetches the
summary of a user-selected article and prepares it for classification by the model.

\subsection{Data Processing}
We removed markup, standardized section headings, and converted text to plain UTF-8 for
tokenization. Articles were formatted into JSON objects containing title, revision ID, and
text fields. Labeled examples were split into training, validation, and test sets.
Supporting preprocessing scripts and utilities are available in our GitHub repository:
\href{https://github.com/arya-kumar1/Generative-LLMs-in-Education}{\texttt{Generative-LLMs-in-Education}}.

\subsection{Analysis Methods}
In \textbf{Track A}, we analyzed relationships between linguistic structure and
educational quality using descriptive statistics and exploratory comparisons (e.g.,
readability indices, sentence length, example density).

In \textbf{Track B}, we trained a chain-of-thought LLM to classify each article as
\emph{educationally effective}, \emph{neutral}, or \emph{ineffective}. The model was tuned
using annotated examples and prompt templates derived from instructional design theory.
Evaluation used standard metrics—accuracy, precision, recall, and F1—against expert labels.
A subset of model predictions was qualitatively reviewed by educators to assess reasoning
clarity.

\subsection{Integration and Reproducibility}
Outputs from Track A were used to guide prompt construction in Track B, ensuring that the
model’s reasoning aligns with empirically validated text features. All code, scripts, and
documentation for data collection, preprocessing, and analysis are publicly available at
\href{https://github.com/arya-kumar1/Generative-LLMs-in-Education}{\texttt{github.com/arya-kumar1/Generative-LLMs-in-Education}},
including a detailed README describing runtime dependencies, API keys, and environment setup.

\subsection{Ethical Considerations}
All data consist of publicly available Wikipedia content accessed under the Wikimedia
\href{https://foundation.wikimedia.org/wiki/Policy:Terms_of_Use}{Terms of Use}.
No private or personally identifiable information was collected. Human evaluations were
conducted anonymously, and rater identifiers were not stored.
