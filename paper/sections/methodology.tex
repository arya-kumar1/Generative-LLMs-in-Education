\section{Methodology}
\label{sec:methodology}

\subsection{Overview}
This project is divided into two main tracks that work together to explore how AI can evaluate the educational quality of Wikipedia articles. \textbf{Track A} investigates how linguistic and structural features relate to effective learning. \textbf{Track B} develops and tests a large language model (LLM) that can automatically classify articles based on their educational usefulness. Insights from both tracks are combined to build an AI system that can explain why an article may or may not be effective for learning.

\subsection{Data Collection}
For \textbf{Track A}, we conducted a literature review focused on instructional text design—specifically looking at coherence, scaffolding, conceptual density, and explanatory depth. Based on this review, we compiled a labeled dataset of Wikipedia articles that have been evaluated for educational quality by experts or previous research. Each article’s revision ID and access date were recorded to ensure reproducibility.

For \textbf{Track B}, we used the Wikipedia API in combination with the OpenAI Apps SDK to retrieve article content in real time. The program fetches summaries of user-selected Wikipedia articles and prepares them for model input. This allows the system to dynamically analyze any article the user is interested in.

\subsection{Data Processing}
After collection, the articles were cleaned and formatted for consistency. We removed unnecessary markup, standardized section headings, and organized the data into labeled examples for model training. This process ensured that both the linguistic and structural features of each article could be accurately analyzed by the LLM.

\subsection{Analysis Methods}
In \textbf{Track A}, we examined which linguistic and structural features—such as readability, sentence complexity, and organization—were most closely associated with higher educational quality. These features were identified through existing research and the labeled Wikipedia dataset.

In \textbf{Track B}, we trained and refined a chain-of-thought (CoT) LLM to classify articles as educationally effective, neutral, or ineffective. The model was fine-tuned using annotated examples and prompt engineering to improve its ability to explain reasoning behind each classification.

To evaluate the model, we compared its predictions to expert labels using standard metrics such as accuracy, precision, and F1 score. A subset of model outputs was also reviewed by educators to qualitatively assess reasoning clarity and alignment with human judgment.

\subsection{Integration}
After both tracks were completed, we integrated the findings. The linguistic and structural insights from Track A informed the model’s reasoning process in Track B, allowing the final system to not only score article quality but also provide an interpretable explanation for its decision.

\subsection{Deliverables}
The project results include:
\begin{itemize}
    \item A written report summarizing the linguistic and educational factors that make instructional writing more effective.
    \item A functional prototype, the \emph{Wiki Diagnostic LLM}, which connects to the Wikipedia API via the OpenAI Apps SDK and evaluates the educational suitability of articles in real time.
\end{itemize}