\input{methodology} % If you prefer, paste the full section below instead of \input.

\section{Methodology}
\label{sec:methodology}

\subsection*{Overview}
We follow a two–track design. \textbf{Track A} identifies linguistic/structural correlates of educational quality via literature-driven feature engineering and empirical analysis. \textbf{Track B} implements a CoT-enabled “Wiki Diagnostic Tutor” that retrieves Wikipedia content via API and classifies articles with explanations. An additional Week-7 study evaluates the educational suitability of lead summaries using a simple clarity–comprehensiveness–readability rubric.

\subsection{Data Collection}
\textbf{Track A.} We assembled a labeled set of Wikipedia articles rated for educational quality by prior research or expert raters (with revision IDs and access dates). We also compiled a review corpus on instructional quality, writing attitudes, and critical thinking \cite{ekholm2018clarifying, pithers2000critical, fadhly2022efl}. Articles were fetched via the Wikimedia REST API summary and content endpoints \cite{wikimediaREST}, with request metadata logged.

\textbf{Track B.} The prototype connects to the Wikipedia API using the OpenAI Apps SDK tool interface to fetch the page summary (and optionally full text) at inference time \cite{openaiAppsSDK}. 

\textbf{Week-7 Summaries Study.} We executed \texttt{wiki\_summarizer.py} on 5–10 topics (e.g., “AI,” “Machine learning,” “Neural network,” “Overfitting,” “Photosynthesis”) and recorded printed summaries and rubric scores (clarity, comprehensiveness, readability; 1–5 scale), producing per-topic and average scores.

\subsection{Data Processing}
We stripped markup and references, preserved section hierarchy, and tokenized sentences/words. We computed readability (Flesch–Kincaid, Coleman–Liau), hierarchy depth, mean sentences per section, example density (examples per 1{,}000 words), discourse cues (connectives), and lexical diversity. For labels from multiple sources, two expert raters adjudicated disagreements; inter-rater reliability was assessed with Cohen’s~$\kappa$.

\subsection{Analysis Methods}
\textbf{Track A (Feature Discovery).} We tested associations between features and labels using ANOVA/Kruskal–Wallis and logistic regression with FDR correction; we report effect sizes (Cohen’s~$d$, Cliff’s~$\delta$) and domain robustness.

\textbf{Track B (Modeling).} We frame tri-class classification with CoT rationales. The system uses: (i) a Wikipedia retrieval tool; (ii) few-shot CoT prompts emphasizing coherence, scaffolding, example use, and hierarchy; (iii) instruction-tuning on annotated examples. Splits are stratified (70/15/15) at the article level.

\textbf{Evaluation.} Automated metrics include accuracy, macro-precision/recall/F1 with 95\% bootstrap CIs. Human experts blind-rate a 10\% sample for validity and reasoning clarity; we compute Spearman~$\rho$ and $\kappa$. Ablations remove Track-A-inspired cues to quantify contribution deltas.

\subsection{Week-7 Rubric Results (Baseline)}
Average summary score across topics $\approx 4.1$/5 (\emph{high suitability}). The most effective summaries followed a “definition $+$ key ideas/applications” pattern (e.g., AI, Photosynthesis), suggesting lead summaries can serve as early indicators for article-level educational quality.

\subsection{System and Reproducibility}
All scripts (preprocessing, modeling, evaluation) are version-controlled with fixed seeds and an environment lockfile (Python~3.11; \texttt{pandas}, \texttt{numpy}, \texttt{scikit-learn}, \texttt{spacy}, \texttt{textstat}). We log prompts, tool I/O (titles, revision IDs), and outputs. API use complies with Wikimedia Terms of Use; we respect rate limits.

\subsection{Ethical Considerations}
Data are public Wikipedia content accessed under the Wikimedia Terms of Use. We collect minimum necessary metadata, anonymize rater IDs, and release revision IDs for reproducibility.
